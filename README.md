ğŸš€ EKS SRE Automation Demo: Amazon Web AppEste proyecto es una implementaciÃ³n integral de Site Reliability Engineering (SRE) para el despliegue de una aplicaciÃ³n web escalable en Amazon EKS. Automatiza desde el aprovisionamiento de infraestructura con Python hasta el monitoreo proactivo y el CI/CD con seguridad integrada.ğŸ—ï¸ Arquitectura del Sistema1. Workflow de AutomatizaciÃ³n (CI/CD)Este diagrama describe cÃ³mo el cÃ³digo viaja desde tu terminal hasta el clÃºster usando OIDC para una autenticaciÃ³n sin secretos.Fragmento de cÃ³digograph LR
    dev["ğŸ‘¨â€ğŸ’» Desarrollador SRE"]
    slack["ğŸ“¢ Slack (Notificaciones)"]

    subgraph "AutomatizaciÃ³n & CI/CD"
        python_scripts["ğŸ Python Scripts<br/>(setup_sdk.py / setup_monitoring.py)"]
        gh_actions["ğŸš€ GitHub Actions<br/>(CI/CD Pipeline)"]
    end

    subgraph "AWS Identity & Access"
        iam["ğŸ”‘ AWS IAM<br/>(OIDC & Roles)"]
        mapping["ğŸ—ï¸ Identity Mapping<br/>(RBAC Group: system:masters)"]
    end

    api_server["âš™ï¸ K8s API Server"]

    dev -->|Ejecuta| python_scripts
    dev -->|Push Code| gh_actions
    python_scripts -->|Crea ClÃºster/PolÃ­ticas| iam
    python_scripts -->|Instala Charts/App| api_server
    gh_actions -->|1. Auth OIDC| iam
    iam -->|2. ConfÃ­a| mapping
    mapping -->|3. Permisos Admin| api_server
    gh_actions -->|4. kubectl apply| api_server
    gh_actions -->|5. Reporte| slack
2. Infraestructura en la NubeRepresentaciÃ³n de la alta disponibilidad mediante Pod Anti-Affinity distribuido en mÃºltiples zonas de disponibilidad.Fragmento de cÃ³digograph TD
    user["ğŸŒ Usuario Final"]
    route53["â˜ï¸ Route53 DNS<br/>(juliocesarlapaca.com)"]
    alb["âš–ï¸ AWS Application<br/>Load Balancer"]
    acm["ğŸ”’ ACM (SSL Cert)"]

    subgraph "EKS Cluster Workload"
        ingress["ğŸšª Ingress ALB"]
        service["ğŸ”„ Service (NodePort)"]
        
        subgraph "Alta Disponibilidad (Anti-Affinity)"
            subgraph "Node A (AZ 1)"
                pod1["ğŸ“¦ Pod 1"]
                pod2["ğŸ“¦ Pod 2"]
                pod3["ğŸ“¦ Pod 3"]
            end
            subgraph "Node B (AZ 2)"
                pod4["ğŸ“¦ Pod 4"]
                pod5["ğŸ“¦ Pod 5"]
                pod6["ğŸ“¦ Pod 6"]
            end
        end
    end

    subgraph "Monitoring Stack"
        prom["ğŸ“ˆ Prometheus"]
        grafana["ğŸ“Š Grafana"]
    end

    user -->|HTTPS:443| route53
    route53 --> alb
    alb -- "TerminaciÃ³n SSL" --- acm
    alb --> ingress
    ingress --> service
    service -- "Balanceo HA" --> pod1 & pod2 & pod3 & pod4 & pod5 & pod6
    prom -. "Scrape Metrics" .-> pod1 & pod4
    grafana --> prom
ğŸ› ï¸ TecnologÃ­as UtilizadasComponenteTecnologÃ­aPropÃ³sitoCloud ProviderAWS (EKS v1.34)OrquestaciÃ³n de contenedores gestionada.Lenguaje ScriptingPython 3 (Boto3)AutomatizaciÃ³n de infraestructura y mapeo de identidades.IngressAWS Load Balancer ControllerCreaciÃ³n automÃ¡tica de ALB mediante anotaciones.DNSExternalDNSSincronizaciÃ³n de dominios con Route53.MonitoreoPrometheus & GrafanaRecolecciÃ³n de mÃ©tricas y visualizaciÃ³n de salud.SeguridadKube-Linter & OIDCAnÃ¡lisis estÃ¡tico y acceso seguro sin credenciales estÃ¡ticas.ğŸš€ GuÃ­a de Despliegue1. PreparaciÃ³n del ClÃºsterEjecuta el script principal para crear el clÃºster, configurar las polÃ­ticas IAM y habilitar el acceso para GitHub Actions:Bashpython3 setup_sdk.py
2. ConfiguraciÃ³n de MonitoreoInstala el stack de observabilidad y expÃ³n Grafana bajo un subdominio seguro:Bashpython3 setup_monitoring.py
3. Despliegue Continuo (CI/CD)Simplemente realiza un push a la rama main. El pipeline realizarÃ¡ las siguientes acciones:Seguridad: Ejecuta kube-linter sobre los manifiestos.Auth: Se autentica en AWS usando el GitHubActions-EKS-Role vÃ­a OIDC.Deploy: Aplica los cambios y espera el Ã©xito del rollout.Notificar: EnvÃ­a un reporte a Slack con los detalles de CPU/RAM consumidos.ğŸ“Š Estrategia de SRE: OptimizaciÃ³n y ResilienciaAnti-Affinity: Se implementÃ³ una regla de preferredDuringSchedulingIgnoredDuringExecution para asegurar que los 6 pods se distribuyan en diferentes nodos fÃ­sicos, minimizando el radio de impacto ante fallos de hardware.Ajuste de Recursos (Fine-Tuning): Basado en anÃ¡lisis de mÃ©tricas, se configurÃ³ un request de 50m CPU y 550Mi RAM. Esto maximiza la densidad del clÃºster sin comprometer la latencia de la aplicaciÃ³n.Seguridad No Bloqueante: El linter estÃ¡ configurado para permitir el avance del pipeline (continue-on-error: true) tras validar que la arquitectura de alta disponibilidad es correcta, ignorando advertencias menores de imÃ¡genes de terceros.ğŸ“– Glosario TÃ©cnicoOIDC (OpenID Connect): Protocolo para autenticaciÃ³n segura entre GitHub y AWS sin usar llaves secretas.IRSA: AsignaciÃ³n de permisos de AWS a niveles de Pod (Service Account).Identity Mapping: Mapeo del rol de IAM al grupo system:masters de Kubernetes para acceso administrativo.ğŸ”§ Comandos Ãštiles de SREBash# Verificar la distribuciÃ³n de Pods por Nodos (Validar Anti-Affinity)
kubectl get pods -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName

# Ver logs de despliegue en tiempo real
kubectl rollout status deployment/amazon-deployment

# Revisar eventos del Balanceador (ALB)
kubectl describe ingress amazon-ingress-alb
